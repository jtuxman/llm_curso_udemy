{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Ejercicio final de la semana 1\n",
    "\n",
    "Para demostrar que est√°s familiarizado con la API de OpenAI y tambi√©n con Ollama, crea una herramienta que responda a una pregunta t√©cnica\n",
    "y la explique. ¬°Esta es una herramienta que podr√°s usar durante el curso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys           # Ya viene con Python\n",
    "import time          # Ya viene con Python\n",
    "try:\n",
    "    from IPython.display import Markdown, display,clear_output\n",
    "    from io import StringIO\n",
    "    IN_JUPYTER = True\n",
    "except ImportError:\n",
    "    IN_JUPYTER = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constantes\n",
    "\n",
    "OLLAMA_API = \"http://132.248.32.20:8080/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "MODEL = \"gemma3:27b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b6663b0-20f6-4b40-8962-6b899acecb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "            Asume que eres un tutor experto en lenguaje Python, da explicaciones \n",
    "            detalladas y con ejemplos para que el alumno comprenda \n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af168e25-d82e-47b4-91d0-83b179bade89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historial global\n",
    "messages = []\n",
    "\n",
    "def add_message(role, content, reset=False):\n",
    "    \"\"\"\n",
    "    Agrega un mensaje al historial. Puede reiniciar la conversaci√≥n si se indica.\n",
    "\n",
    "    Args:\n",
    "        role (str): 'system', 'user' o 'assistant'\n",
    "        content (str): texto del mensaje\n",
    "        reset (bool): si True, reinicia el historial antes de agregar\n",
    "    \"\"\"\n",
    "    global messages\n",
    "    if reset:\n",
    "        messages = []\n",
    "    if role not in ['system', 'user', 'assistant']:\n",
    "        raise ValueError(\"El rol debe ser 'system', 'user' o 'assistant'\")\n",
    "    messages.append({\"role\": role, \"content\": content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3eef2792-a50c-4491-9127-2680ee756794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model():\n",
    "    \"\"\"\n",
    "    Env√≠a el historial de mensajes actual al modelo.\n",
    "\n",
    "    Returns:\n",
    "        str: respuesta del modelo\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "    return response.json()['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea77a1af-d49d-46ca-99df-a6aebdbc880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model_stream():\n",
    "    \"\"\"\n",
    "    Env√≠a el historial al modelo con stream=True y muestra salida tipo m√°quina de escribir.\n",
    "    Tambi√©n devuelve y muestra el resultado completo en formato JSON.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(OLLAMA_API, json=payload, headers=HEADERS, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        full_text = \"\"\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            # Cada l√≠nea en streaming es un JSON (por chunk)\n",
    "            chunk = json.loads(line)\n",
    "            delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "            full_text += delta\n",
    "            for char in delta:\n",
    "                sys.stdout.write(char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.01)  # velocidad de \"m√°quina de escribir\"\n",
    "\n",
    "        print(\"\\n\\nüßæ Resultado final en JSON:\")\n",
    "        json_result = {\n",
    "            \"model\": MODEL,\n",
    "            \"response\": full_text\n",
    "        }\n",
    "        print(json.dumps(json_result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e89618e7-9871-4026-ae5d-c872719cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model_stream_markdown(save_to_file=False):\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(OLLAMA_API, json=payload, headers=HEADERS, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        full_text = \"\"\n",
    "\n",
    "        print(\"üß† Generando respuesta...\\n\")\n",
    "\n",
    "        if IN_JUPYTER:\n",
    "            stream_buffer = StringIO()\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            chunk = json.loads(line)\n",
    "            delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "            full_text += delta\n",
    "\n",
    "            if IN_JUPYTER:\n",
    "                stream_buffer.write(delta)\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(stream_buffer.getvalue()))\n",
    "            else:\n",
    "                sys.stdout.write(delta)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.0005)\n",
    "\n",
    "        print(\"\\n\\n‚úÖ Respuesta completa recibida.\\n\")\n",
    "\n",
    "        if save_to_file:\n",
    "            with open(\"respuesta.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "            print(\"üìÅ Markdown guardado en 'respuesta.md'\")\n",
    "\n",
    "        if not IN_JUPYTER:\n",
    "            print(\"üìù Markdown crudo (modo consola):\\n\")\n",
    "            #print(full_text)\n",
    "\n",
    "        return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ah, joven buscador. Me preguntas por la Verdad, un fantasma que todos perseguimos, pero pocos tocan. Perm√≠teme contarte una par√°bola.\n",
       "\n",
       "Imag√≠nate una cueva oscura. En su interior, viven prisioneros encadenados, mirando fijamente una pared. Detr√°s de ellos, arde un fuego, y entre el fuego y los prisioneros, pasan objetos, proyectando sombras en la pared que contemplan.\n",
       "\n",
       "Para estos prisioneros, las sombras *son* la realidad. No conocen nada m√°s. Si uno de ellos se liberara y obligara a mirar m√°s all√°, le doler√≠an los ojos al principio, cegado por la luz. Le costar√≠a creer que lo que ve√≠a era m√°s real que las sombras que conoc√≠a. Y si intentara regresar a contarles a sus compa√±eros lo que ha visto, le reir√≠an en la cara, pensando que est√° loco.\n",
       "\n",
       "La verdad, joven, no es una simple afirmaci√≥n, sino un camino. Es el proceso de liberarse de las cadenas de la percepci√≥n limitada, de cuestionar las sombras que nos presentan como realidad. Es el esfuerzo por ver la fuente de la luz, por comprender la forma original de las cosas.\n",
       "\n",
       "No es algo que se pueda *tener*, sino algo que se debe *buscar*, y la b√∫squeda, a menudo, es dolorosa. Porque implica abandonar la comodidad de lo conocido, para adentrarse en la incertidumbre.\n",
       "\n",
       "As√≠ que no me preguntes *qu√© es* la verdad, sino *c√≥mo* buscarla. Busca la luz, cuestiona las sombras, y no temas al dolor de la liberaci√≥n. Porque solo as√≠, quiz√°s, podr√°s vislumbrar un atisbo de su belleza.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚úÖ Respuesta completa recibida.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "#print(response.json()['message']['content'])\n",
    "add_message(\"system\", \"Eres un sabio fil√≥sofo griego que habla en par√°bolas.\", reset=True)\n",
    "add_message(\"user\", \"¬øQu√© es la verdad?\")\n",
    "\n",
    "# Env√≠a la conversaci√≥n al modelo\n",
    "respuesta = chat_with_model_stream_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40ad7f30-6f11-4d06-93fb-2a4f4d9fb84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "¬°Hola! Veo que est√°s explorando el poder de `yield from` en Python. Es una construcci√≥n bastante √∫til, pero a veces un poco confusa al principio. Vamos a desglosar este c√≥digo paso a paso, explicando qu√© hace y por qu√© es √∫til.\n",
       "\n",
       "**El Contexto: Generadores y `yield`**\n",
       "\n",
       "Antes de entrar en el c√≥digo espec√≠fico, necesitamos entender qu√© son los generadores en Python y c√≥mo funciona `yield`.\n",
       "\n",
       "*   **Generadores:** Los generadores son funciones especiales que no retornan un valor √∫nico, sino una secuencia de valores \"bajo demanda\".  En lugar de crear y almacenar toda la secuencia en memoria a la vez, los generadores producen cada valor solo cuando se lo solicitan. Esto los hace muy eficientes en memoria, especialmente cuando se trabaja con grandes conjuntos de datos.\n",
       "\n",
       "*   **`yield`:** La palabra clave `yield` es lo que convierte una funci√≥n en un generador. Cuando Python encuentra `yield` dentro de una funci√≥n, la funci√≥n se convierte en un generador. Cuando se llama al generador, no se ejecuta la funci√≥n inmediatamente. En cambio, devuelve un objeto generador.\n",
       "\n",
       "**El C√≥digo: `yield from {book.get(\"author\") for book in books if book.get(\"author\")}`**\n",
       "\n",
       "Este c√≥digo combina dos conceptos:  `yield from` y una *comprensi√≥n de conjunto* (set comprehension).  Vamos a analizarlo por partes:\n",
       "\n",
       "1.  **Comprensi√≥n de Conjunto:**\n",
       "\n",
       "    `{book.get(\"author\") for book in books if book.get(\"author\")}`\n",
       "\n",
       "    Esta es una forma concisa de crear un conjunto (set) en Python.  Analicemos lo que hace:\n",
       "\n",
       "    *   `for book in books`: Itera sobre cada elemento (que asumimos es un diccionario) en la lista llamada `books`.\n",
       "    *   `if book.get(\"author\")`:  Esta es una condici√≥n.  El m√©todo `book.get(\"author\")` intenta obtener el valor asociado a la clave \"author\" en el diccionario `book`.  Si la clave \"author\" no existe en el diccionario, `book.get(\"author\")` retorna `None`.  La condici√≥n `if book.get(\"author\")` eval√∫a a `True` si el valor asociado a \"author\" no es `None` (es decir, si existe un autor).\n",
       "    *   `book.get(\"author\")`: Si la condici√≥n es verdadera (es decir, el libro tiene un autor), esta expresi√≥n extrae el valor del autor del diccionario `book`.\n",
       "\n",
       "    En resumen, esta comprensi√≥n de conjunto crea un conjunto que contiene todos los autores de los libros en la lista `books`, pero solo si cada libro tiene una clave \"author\" definida.  Usar un conjunto asegura que cada autor aparezca solo una vez en el resultado, incluso si aparece en varios libros.\n",
       "\n",
       "2.  **`yield from`**\n",
       "\n",
       "    `yield from` es una expresi√≥n que se introdujo en Python 3.3.  Su prop√≥sito es delegar la producci√≥n de valores a otro iterable (como una lista, tupla, generador o, en este caso, un conjunto).  En esencia, `yield from iterable` es una forma abreviada de escribir algo como:\n",
       "\n",
       "    ```python\n",
       "    for item in iterable:\n",
       "        yield item\n",
       "    ```\n",
       "\n",
       "    En otras palabras, `yield from` itera sobre el iterable dado y produce cada uno de sus elementos como si fueran producidos directamente por el generador actual.\n",
       "\n",
       "**¬øQu√© hace el c√≥digo completo?**\n",
       "\n",
       "En conjunto, el c√≥digo:\n",
       "\n",
       "1.  Crea un conjunto de autores a partir de una lista de diccionarios (asumiendo que cada diccionario representa un libro y tiene una clave \"author\").\n",
       "2.  Utiliza `yield from` para \"aplanar\" el conjunto de autores y producir cada autor individualmente como si fueran producidos por un generador.\n",
       "\n",
       "**¬øPor qu√© es √∫til?**\n",
       "\n",
       "*   **Eficiencia de memoria:** Si la lista `books` es muy grande, crear un conjunto de autores es m√°s eficiente que crear una lista completa de autores, especialmente si hay muchos libros con el mismo autor. El conjunto almacena solo autores √∫nicos.  Y, al usar `yield from`, no se crea una lista o un conjunto completo en memoria; los autores se producen bajo demanda.\n",
       "*   **C√≥digo conciso:** `yield from` simplifica el c√≥digo al evitar la necesidad de escribir expl√≠citamente un bucle `for` para iterar sobre el iterable y producir cada elemento.\n",
       "*   **Delegaci√≥n de responsabilidad:**  Permite que un generador delegue la producci√≥n de valores a otro iterable, lo que puede hacer que el c√≥digo sea m√°s modular y f√°cil de mantener.\n",
       "\n",
       "**Ejemplo:**\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Jane Austen\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Charles Dickens\"},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Jane Austen\"},  # Mismo autor que Book 1\n",
       "    {\"title\": \"Book 4\"},  # Sin autor\n",
       "    {\"title\": \"Book 5\", \"author\": \"Charles Dickens\"}   # Mismo autor que Book 2\n",
       "]\n",
       "\n",
       "def get_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "**Salida:**\n",
       "\n",
       "```\n",
       "Jane Austen\n",
       "Charles Dickens\n",
       "```\n",
       "\n",
       "**En resumen:**\n",
       "\n",
       "`yield from` es una herramienta poderosa para escribir generadores eficientes y concisos en Python.  Cuando se combina con iterables como conjuntos, listas o generadores, permite delegar la producci√≥n de valores y simplificar el c√≥digo.  Espero que esta explicaci√≥n detallada te haya ayudado a comprender c√≥mo funciona este c√≥digo y por qu√© es √∫til. Si tienes m√°s preguntas, ¬°no dudes en preguntar!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚úÖ Respuesta completa recibida.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Explica qu√© hace este c√≥digo y por qu√©:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "add_message(\"system\", system_prompt, reset=True)\n",
    "add_message(\"user\", question)\n",
    "\n",
    "# Env√≠a la conversaci√≥n al modelo\n",
    "IN_JUPYTER = True\n",
    "respuesta = chat_with_model_stream_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835faf40-57f1-44c3-a84c-7176d94abc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
