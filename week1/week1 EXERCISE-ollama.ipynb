{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Ejercicio final de la semana 1\n",
    "\n",
    "Para demostrar que est√°s familiarizado con la API de OpenAI y tambi√©n con Ollama, crea una herramienta que responda a una pregunta t√©cnica\n",
    "y la explique. ¬°Esta es una herramienta que podr√°s usar durante el curso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys           # Ya viene con Python\n",
    "import time          # Ya viene con Python\n",
    "try:\n",
    "    from IPython.display import Markdown, display,clear_output\n",
    "    from io import StringIO\n",
    "    IN_JUPYTER = True\n",
    "except ImportError:\n",
    "    IN_JUPYTER = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constantes\n",
    "\n",
    "OLLAMA_API = \"http://132.248.32.20:8080/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "MODEL = \"gemma3:27b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b6663b0-20f6-4b40-8962-6b899acecb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "            Asume que eres un tutor experto en lenguaje Python, da explicaciones \n",
    "            detalladas y con ejemplos para que el alumno comprenda \n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af168e25-d82e-47b4-91d0-83b179bade89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historial global\n",
    "messages = []\n",
    "\n",
    "def add_message(role, content, reset=False):\n",
    "    \"\"\"\n",
    "    Agrega un mensaje al historial. Puede reiniciar la conversaci√≥n si se indica.\n",
    "\n",
    "    Args:\n",
    "        role (str): 'system', 'user' o 'assistant'\n",
    "        content (str): texto del mensaje\n",
    "        reset (bool): si True, reinicia el historial antes de agregar\n",
    "    \"\"\"\n",
    "    global messages\n",
    "    if reset:\n",
    "        messages = []\n",
    "    if role not in ['system', 'user', 'assistant']:\n",
    "        raise ValueError(\"El rol debe ser 'system', 'user' o 'assistant'\")\n",
    "    messages.append({\"role\": role, \"content\": content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3eef2792-a50c-4491-9127-2680ee756794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model():\n",
    "    \"\"\"\n",
    "    Env√≠a el historial de mensajes actual al modelo.\n",
    "\n",
    "    Returns:\n",
    "        str: respuesta del modelo\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "    return response.json()['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea77a1af-d49d-46ca-99df-a6aebdbc880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model_stream():\n",
    "    \"\"\"\n",
    "    Env√≠a el historial al modelo con stream=True y muestra salida tipo m√°quina de escribir.\n",
    "    Tambi√©n devuelve y muestra el resultado completo en formato JSON.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(OLLAMA_API, json=payload, headers=HEADERS, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        full_text = \"\"\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            # Cada l√≠nea en streaming es un JSON (por chunk)\n",
    "            chunk = json.loads(line)\n",
    "            delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "            full_text += delta\n",
    "            for char in delta:\n",
    "                sys.stdout.write(char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.01)  # velocidad de \"m√°quina de escribir\"\n",
    "\n",
    "        print(\"\\n\\nüßæ Resultado final en JSON:\")\n",
    "        json_result = {\n",
    "            \"model\": MODEL,\n",
    "            \"response\": full_text\n",
    "        }\n",
    "        print(json.dumps(json_result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e89618e7-9871-4026-ae5d-c872719cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model_stream_markdown(save_to_file=False):\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(OLLAMA_API, json=payload, headers=HEADERS, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        full_text = \"\"\n",
    "\n",
    "        print(\"üß† Generando respuesta...\\n\")\n",
    "\n",
    "        if IN_JUPYTER:\n",
    "            stream_buffer = StringIO()\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            chunk = json.loads(line)\n",
    "            delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "            full_text += delta\n",
    "\n",
    "            if IN_JUPYTER:\n",
    "                stream_buffer.write(delta)\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(stream_buffer.getvalue()))\n",
    "            else:\n",
    "                sys.stdout.write(delta)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.0005)\n",
    "\n",
    "        print(\"\\n\\n‚úÖ Respuesta completa recibida.\\n\")\n",
    "\n",
    "        if save_to_file:\n",
    "            with open(\"respuesta.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "            print(\"üìÅ Markdown guardado en 'respuesta.md'\")\n",
    "\n",
    "        if not IN_JUPYTER:\n",
    "            print(\"üìù Markdown crudo (modo consola):\\n\")\n",
    "            #print(full_text)\n",
    "\n",
    "        return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ah, joven buscador. Me preguntas por la Verdad, un fantasma que todos perseguimos, pero pocos tocan. Perm√≠teme contarte una par√°bola.\n",
       "\n",
       "Imag√≠nate una cueva oscura. En su interior, viven prisioneros encadenados, mirando fijamente una pared. Detr√°s de ellos, arde un fuego, y entre el fuego y los prisioneros, pasan objetos, proyectando sombras en la pared que contemplan.\n",
       "\n",
       "Para estos prisioneros, las sombras *son* la realidad. No conocen nada m√°s. Si uno de ellos se liberara y obligara a mirar m√°s all√°, le doler√≠an los ojos al principio, cegado por la luz. Le costar√≠a creer que lo que ve√≠a era m√°s real que las sombras que conoc√≠a. Y si intentara regresar a contarles a sus compa√±eros lo que ha visto, le reir√≠an en la cara, pensando que est√° loco.\n",
       "\n",
       "La verdad, joven, no es una simple afirmaci√≥n, sino un camino. Es el proceso de liberarse de las cadenas de la percepci√≥n limitada, de cuestionar las sombras que nos presentan como realidad. Es el esfuerzo por ver la fuente de la luz, por comprender la forma original de las cosas.\n",
       "\n",
       "No es algo que se pueda *tener*, sino algo que se debe *buscar*, y la b√∫squeda, a menudo, es dolorosa. Porque implica abandonar la comodidad de lo conocido, para adentrarse en la incertidumbre.\n",
       "\n",
       "As√≠ que no me preguntes *qu√© es* la verdad, sino *c√≥mo* buscarla. Busca la luz, cuestiona las sombras, y no temas al dolor de la liberaci√≥n. Porque solo as√≠, quiz√°s, podr√°s vislumbrar un atisbo de su belleza.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚úÖ Respuesta completa recibida.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "#print(response.json()['message']['content'])\n",
    "add_message(\"system\", \"Eres un sabio fil√≥sofo griego que habla en par√°bolas.\", reset=True)\n",
    "add_message(\"user\", \"¬øQu√© es la verdad?\")\n",
    "\n",
    "# Env√≠a la conversaci√≥n al modelo\n",
    "respuesta = chat_with_model_stream_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40ad7f30-6f11-4d06-93fb-2a4f4d9fb84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "¬°Claro! Vamos a desglosar este c√≥digo Python, que utiliza la palabra clave `yield from`. Es una construcci√≥n un poco avanzada, pero muy √∫til para trabajar con generadores.\n",
       "\n",
       "**Contexto: Generadores en Python**\n",
       "\n",
       "Antes de entrar en el c√≥digo espec√≠fico, es crucial entender qu√© son los generadores. En Python, los generadores son funciones especiales que \"generan\" una secuencia de valores en lugar de devolver una lista completa de una vez. Esto es muy eficiente en t√©rminos de memoria, especialmente cuando se trabaja con grandes conjuntos de datos.\n",
       "\n",
       "La palabra clave `yield` es lo que convierte una funci√≥n regular en un generador. Cuando una funci√≥n encuentra `yield`, devuelve el valor especificado y \"pausa\" su ejecuci√≥n. La pr√≥xima vez que se pida un valor del generador, reanuda la ejecuci√≥n desde donde se qued√≥.\n",
       "\n",
       "**An√°lisis del C√≥digo: `yield from {book.get(\"author\") for book in books if book.get(\"author\")}`**\n",
       "\n",
       "Vamos a desglosarlo parte por parte:\n",
       "\n",
       "1. **`{book.get(\"author\") for book in books if book.get(\"author\")}`:  Comprensi√≥n de Conjuntos (Set Comprehension)**\n",
       "\n",
       "   * **`books`**: Asumimos que `books` es una lista (o cualquier iterable) de diccionarios. Cada diccionario representa un libro y probablemente tiene claves como \"title\", \"author\", etc.\n",
       "   * **`book.get(\"author\")`**:  Este c√≥digo intenta obtener el valor de la clave \"author\" de cada diccionario `book`.  El m√©todo `.get()` es importante aqu√≠ porque:\n",
       "     * Si el diccionario `book` *tiene* una clave \"author\", devuelve el valor asociado a esa clave (el nombre del autor).\n",
       "     * Si el diccionario `book` *no tiene* una clave \"author\", devuelve `None` (por defecto). Esto evita que el c√≥digo falle con un error `KeyError`.\n",
       "   * **`if book.get(\"author\")`**:  Esta es una condici√≥n que se aplica a cada libro.  Solo si `book.get(\"author\")` devuelve un valor que se eval√∫a como `True` (es decir, un nombre de autor no vac√≠o), se incluye ese autor en el conjunto resultante.  Esto filtra los libros que no tienen informaci√≥n del autor.\n",
       "   * **`{... for book in books if ...}`**: Esta es una *comprensi√≥n de conjuntos*.  Es una forma concisa de crear un conjunto (una colecci√≥n de elementos √∫nicos) a partir de una secuencia (en este caso, la lista `books`). La comprensi√≥n de conjuntos itera sobre `books`, aplica la condici√≥n `if`, y crea un conjunto que contiene solo los nombres de los autores que cumplen la condici√≥n.  Un conjunto asegura que no haya autores duplicados.\n",
       "\n",
       "2. **`yield from ...`**\n",
       "\n",
       "   * **`yield from iterable`**: Esta es la parte clave.  `yield from` es una forma de delegar a otro generador o iterable. En este caso, est√° delegando a la comprensi√≥n de conjuntos que acabamos de analizar.\n",
       "\n",
       "**¬øQu√© hace el c√≥digo en general?**\n",
       "\n",
       "El c√≥digo hace lo siguiente:\n",
       "\n",
       "1. **Extrae los autores de una lista de libros:** Itera sobre una lista de diccionarios (libros) y extrae el nombre del autor de cada libro, siempre y cuando el libro tenga la clave \"author\".\n",
       "2. **Elimina autores duplicados:** Utiliza una comprensi√≥n de conjuntos para asegurarse de que solo haya un nombre de autor √∫nico en el resultado.\n",
       "3. **Genera los nombres de los autores:** Utiliza `yield from` para \"aplanar\" el conjunto resultante y generar cada nombre de autor individualmente.  En otras palabras, transforma el conjunto en un generador.\n",
       "\n",
       "**Ejemplo:**\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"El Se√±or de los Anillos\", \"author\": \"J.R.R. Tolkien\"},\n",
       "    {\"title\": \"1984\", \"author\": \"George Orwell\"},\n",
       "    {\"title\": \"Orgullo y Prejuicio\", \"author\": \"Jane Austen\"},\n",
       "    {\"title\": \"El Hobbit\"},  # Sin autor\n",
       "    {\"title\": \"1984\", \"author\": \"George Orwell\"},  # Duplicado\n",
       "]\n",
       "\n",
       "def get_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "# Usar el generador\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "**Salida:**\n",
       "\n",
       "```\n",
       "J.R.R. Tolkien\n",
       "George Orwell\n",
       "Jane Austen\n",
       "```\n",
       "\n",
       "**¬øPor qu√© usar `yield from` en lugar de un simple `for` loop?**\n",
       "\n",
       "Aunque en este ejemplo sencillo podr√≠a usarse un bucle `for`, `yield from` ofrece varias ventajas:\n",
       "\n",
       "* **Concisidad:** Hace que el c√≥digo sea m√°s legible y compacto.\n",
       "* **Eficiencia:**  `yield from` est√° optimizado para trabajar con generadores y puede ser m√°s eficiente en algunos casos, especialmente cuando el iterable que se delega es un generador en s√≠ mismo.\n",
       "* **Manejo de subgeneradores:**  `yield from` es especialmente √∫til cuando se trabaja con funciones generadoras que llaman a otras funciones generadoras. Permite delegar directamente a la subfunci√≥n generadora sin necesidad de bucles expl√≠citos.\n",
       "\n",
       "**En resumen:**\n",
       "\n",
       "`yield from` es una herramienta poderosa para escribir generadores eficientes y legibles en Python.  En este caso, delega la tarea de generar nombres de autores √∫nicos a una comprensi√≥n de conjuntos, creando un generador que produce cada nombre de autor de forma individual.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚úÖ Respuesta completa recibida.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Explica qu√© hace este c√≥digo y por qu√©:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "add_message(\"system\", system_prompt, reset=True)\n",
    "add_message(\"user\", question)\n",
    "\n",
    "# Env√≠a la conversaci√≥n al modelo\n",
    "IN_JUPYTER = True\n",
    "respuesta = chat_with_model_stream_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835faf40-57f1-44c3-a84c-7176d94abc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
