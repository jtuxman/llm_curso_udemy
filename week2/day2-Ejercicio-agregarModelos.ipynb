{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
   "metadata": {},
   "source": [
    "# ¡Ejercicio de Gradio!\n",
    "\n",
    "Agregaremos modelos disponibles de Llama como llama3.2, qwen2.5:7b, gemma3:27b,\n",
    "qwq:latest ademas del gpt-minio\n",
    "\n",
    "\n",
    "## ¡Prepárate para la alegría!\n",
    "\n",
    "Ten en cuenta que las pantallas de Gradio pueden aparecer en \"modo oscuro\" o \"modo claro\" según la configuración de tu computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "#import json\n",
    "import sys           # Ya viene con Python\n",
    "#import time          # Ya viene con Pytho\n",
    "try:\n",
    "    from IPython.display import Markdown, display,clear_output\n",
    "    from io import StringIO\n",
    "    IN_JUPYTER = True\n",
    "except ImportError:\n",
    "    IN_JUPYTER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1715421-cead-400b-99af-986388a97aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key existe y empieza por sk-proj-\n",
      "Anthropic API Key existe y empieza por sk-ant-\n",
      "Google API Key existe y empieza por AIzaSyBz\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuración\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b0595ce-c356-42de-9bd8-79aa411b3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://132.248.32.20:8080/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"ollama3.2\"\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22586021-1795-4929-8079-63f5bb4edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conéctese a OpenAI, Anthropic y Google; comente las líneas de Claude o Google si no las está usando\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b16e6021-6dc4-4397-985a-6679d6c8ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un mensaje genérico del sistema: ¡no más IA adversarias sarcásticas!\n",
    "\n",
    "system_message = \"Eres un asistente útil\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94013d1-4f27-4329-97e8-8c58db93636a",
   "metadata": {},
   "source": [
    "## Momento de crear nuestra interfaz de usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ec007-0314-48bf-84a4-a65943649215",
   "metadata": {},
   "source": [
    "## Forzar el modo oscuro\n",
    "\n",
    "Gradio aparece en modo claro o en modo oscuro según la configuración del navegador y la computadora. Hay una manera de forzar que Gradio aparezca en modo oscuro, pero Gradio recomienda no hacerlo ya que debería ser una preferencia del usuario (particularmente por razones de accesibilidad). Pero si desea forzar el modo oscuro para sus pantallas, a continuación se muestra cómo hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8129afa-532b-4b15-b93c-aa9cca23a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define esta variable y luego pasa js=force_dark_mode al crear la interfaz\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5667cce3-5505-4c23-b7bb-dda158333a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(role, content, reset=False):\n",
    "    \"\"\"\n",
    "    Agrega un mensaje al historial. Puede reiniciar la conversación si se indica.\n",
    "\n",
    "    Args:\n",
    "        role (str): 'system', 'user' o 'assistant'\n",
    "        content (str): texto del mensaje\n",
    "        reset (bool): si True, reinicia el historial antes de agregar\n",
    "    \"\"\"\n",
    "    global messages\n",
    "    if reset:\n",
    "        messages = []\n",
    "    if role not in ['system', 'user', 'assistant']:\n",
    "        raise ValueError(\"El rol debe ser 'system', 'user' o 'assistant'\")\n",
    "    messages.append({\"role\": role, \"content\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear una llamada que transmita resultados\n",
    "# Si deseas un repaso de los generadores (la palabra clave \"yield\"),\n",
    "# Echa un vistazo al cuaderno de Python intermedio en la carpeta de la semana 1.\n",
    "\n",
    "\n",
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbc8e930-ba2a-4194-8f7c-044659150626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49db151a-c300-4276-9922-82c28853326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_ollama():\n",
    "    \"\"\"\n",
    "    Envía el historial al modelo con stream=True y muestra salida tipo máquina de escribir.\n",
    "    También devuelve y muestra el resultado completo en formato JSON.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(OLLAMA_API, json=payload, headers=HEADERS, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        full_text = \"\"\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            # Cada línea en streaming es un JSON (por chunk)\n",
    "            chunk = json.loads(line)\n",
    "            delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "            full_text += delta\n",
    "            yield full_text\n",
    "            for char in delta:\n",
    "                sys.stdout.write(char)\n",
    "                sys.stdout.flush()\n",
    "                #time.sleep(0.01)  # velocidad de \"máquina de escribir\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de0ac3ca-c5aa-4d1d-8b7d-5e845037e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stream_gemini(prompt):    \n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-1.5-flash',\n",
    "        system_instruction=system_message,\n",
    "    )\n",
    "    # Llamada al modelo Gemini con streaming\n",
    "    chat = gemini.start_chat()\n",
    "    response = chat.send_message(prompt, stream=True)\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.text\n",
    "        yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    print (\"modelo \",model)\n",
    "    global MODEL\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    elif model==\"Llama3.2\":\n",
    "        MODEL = \"llama3.2\"\n",
    "        add_message(\"system\", system_message, reset=True)\n",
    "        add_message(\"user\", prompt)\n",
    "        result = stream_ollama()\n",
    "    elif model==\"qwen2.5:7b\":\n",
    "        MODEL = \"qwen2.5:7b\"\n",
    "        add_message(\"system\", system_message, reset=True)\n",
    "        add_message(\"user\", prompt)\n",
    "        result = stream_ollama()\n",
    "    elif model==\"gemma3:27b\":\n",
    "        MODEL = \"gemma3:27b\"\n",
    "        add_message(\"system\", system_message, reset=True)\n",
    "        add_message(\"user\", prompt)\n",
    "        result = stream_ollama()\n",
    "    elif model==\"qwq:latest\":\n",
    "        MODEL = \"qwq:latest\"\n",
    "        add_message(\"system\", system_message, reset=True)\n",
    "        add_message(\"user\", prompt)\n",
    "        result = stream_ollama()\n",
    "    elif model==\"gemini\":\n",
    "        add_message(\"system\", system_message, reset=True)\n",
    "        add_message(\"user\", prompt)\n",
    "        result = stream_gemini(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Modelo Desconocido\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7893\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7893/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Tu mensaje:\"), gr.Dropdown([\"GPT\", \"Llama3.2\",\"qwen2.5:7b\",\"gemma3:27b\",\"qwq:latest\",\"gemini\"], label=\"Selecciona un modelo:\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Respuesta:\")],\n",
    "    js=force_dark_mode,\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede97ca3-a0f8-4f6e-be17-d1de7fef9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"The opposite of hot is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac694d6-6deb-47e8-9421-d1d51a6d99b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
