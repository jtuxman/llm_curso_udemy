{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# ¡Bienvenidos a la Semana 2!\n",
    "\n",
    "## API de modelos Frontier\n",
    "\n",
    "En la Semana 1, usamos múltiples LLM de Frontier a través de su interfaz de chat y nos conectamos con la API de OpenAI.\n",
    "\n",
    "Hoy nos conectaremos con las API de Anthropic y Google, así como también con OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Nota importante: léame</h2>\n",
    "<span style=\"color:#900;\">Estoy mejorando continuamente estos laboratorios, agregando más ejemplos y ejercicios.\n",
    "Al comienzo de cada semana, vale la pena verificar que tenga el código más reciente.<br/>\n",
    "Primero, haga un <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull y combine los cambios según sea necesario</a>. ¿Tiene algún problema? Intente preguntarle a ChatGPT para que le aclare cómo realizar la fusión, o póngase en contacto conmigo.<br/><br/>\n",
    "Después de haber obtenido el código, desde el directorio llm_engineering, en un indicador de Anaconda (PC) o Terminal (Mac), ejecute:<br/>\n",
    "<code>conda env update --f environment.yml --prune</code><br/>\n",
    "O si utilizó virtualenv en lugar de Anaconda, ejecute esto desde su entorno activado en un Powershell (PC) o Terminal (Mac):<br/>\n",
    "<code>pip install -r requirements.txt</code>\n",
    "<br/>Luego reinicie el kernel (menú Kernel >> Reiniciar kernel y borrar resultados de todas las celdas) para incorporar los cambios.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#f71;\">Recordatorio sobre la página de recursos</h2>\n",
    "<span style=\"color:#f71;\">A continuación, se incluye un enlace a los recursos del curso. Esto incluye enlaces a todas las diapositivas.<br/>\n",
    "<a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "Por favor, mantén este artículo en tus favoritos y seguiré agregando más enlaces útiles allí con el tiempo.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Configuración de las claves\n",
    "\n",
    "Si aún no lo ha hecho, ahora puede crear claves API para Anthropic y Google además de OpenAI.\n",
    "\n",
    "**Nota:** si prefiere evitar costos adicionales de API, ¡no dude en omitir la configuración de Anthopic y Google! Puede verme hacerlo y concentrarse en OpenAI para el curso. También puede sustituir Anthropic o Google por Ollama, utilizando el ejercicio que realizó en la semana 1.\n",
    "\n",
    "Para OpenAI, visite https://openai.com/api/\n",
    "Para Anthropic, visite https://console.anthropic.com/\n",
    "Para Google, visite https://ai.google.dev/gemini-api\n",
    "\n",
    "Cuando obtenga sus claves API, debe configurarlas como variables de entorno agregándolas a su archivo `.env`.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Luego, es posible que tengas que reiniciar el kernel de Jupyter Lab (el proceso de Python que se encuentra detrás de este cuaderno) a través del menú Kernel y, luego, volver a ejecutar las celdas desde la parte superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación para Google\n",
    "# En casos excepcionales, esto parece generar un error en algunos sistemas. Comuníquese conmigo si esto sucede.\n",
    "# O puede omitir Gemini: es la prioridad más baja de los modelos de Frontier que usamos.\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key existe y empieza por sk-proj-\n",
      "Anthropic API Key existe y empieza por sk-ant-\n",
      "Google API Key existe y empieza por AIzaSyBz\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuración\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conéctate a OpenAI, Anthropic y Google\n",
    "# Las 3 API son similares\n",
    "# ¿Tienes problemas con los archivos API? Puedes usar openai = OpenAI(api_key=\"your-key-here\") y lo mismo para Claude\n",
    "# ¿Tienes problemas con la configuración de Google Gemini? Entonces, omite Gemini; obtendrás toda la experiencia que necesitas de GPT y Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Pedir a los LLM que cuenten un chiste\n",
    "\n",
    "¡Resulta que los LLM no son muy buenos para contar chistes! Comparemos algunos modelos.\n",
    "\n",
    "Más adelante, les daremos un mejor uso a los LLM.\n",
    "\n",
    "### Qué información se incluye en la API\n",
    "\n",
    "Normalmente, pasaremos a la API:\n",
    "- El nombre del modelo que se debe utilizar\n",
    "- Un mensaje del sistema que brinda un contexto general para el rol que desempeña el LLM\n",
    "- Un mensaje del usuario que brinda el mensaje real\n",
    "\n",
    "Hay otros parámetros que se pueden usar, incluida la **temperatura**, que generalmente está entre 0 y 1; más alta para una salida más aleatoria; más baja para una salida más enfocada y determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Eres un asistente que es genial contando chistes.\"\n",
    "user_prompt = \"Cuente un chiste divertido para una audiencia de científicos de datos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Aquí tienes uno:\n",
      "\n",
      "¿Por qué el científico de datos llevó un paracaídas a la oficina?\n",
      "\n",
      "Por si los datos caen en picada. ¡Ja ja ja!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Aquí va uno:\n",
      "\n",
      "¿Por qué los científicos de datos nunca discuten con los datos?\n",
      "\n",
      "¡Porque siempre tienen la última palabra!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# El ajuste de la temperatura controla la creatividad\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué los científicos de datos siempre están tranquilos?\n",
      "\n",
      "Porque tienen demasiados modelos para lidiar con el caos.\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Claude 3.5 Sonnet\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# La API necesita que el mensaje del sistema se proporcione por separado del mensaje del usuario\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# También se agregaron max_tokens\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m message = \u001b[43mclaude\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaude-3-5-sonnet-20240620\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(message.content[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_utils/_utils.py:275\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/resources/messages/messages.py:953\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    947\u001b[39m     warnings.warn(\n\u001b[32m    948\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    949\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    950\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    951\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_base_client.py:1336\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1323\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1324\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1331\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1332\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1333\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1334\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1335\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_base_client.py:1013\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1011\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_base_client.py:1117\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1114\u001b[39m         err.response.read()\n\u001b[32m   1116\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1120\u001b[39m     cast_to=cast_to,\n\u001b[32m   1121\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1126\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# La API necesita que el mensaje del sistema se proporcione por separado del mensaje del usuario\n",
    "# También se agregaron max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Claude 3.5 Sonnet otra vez\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Ahora agreguemos los resultados en streaming\u001b[39;00m\n\u001b[32m      4\u001b[39m result = claude.messages.stream(\n\u001b[32m      5\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mclaude-3-5-sonnet-20240620\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     max_tokens=\u001b[32m200\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     ],\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/lib/streaming/_messages.py:149\u001b[39m, in \u001b[36mMessageStreamManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> MessageStream:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     raw_stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__api_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mself\u001b[39m.__stream = MessageStream(raw_stream)\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_base_client.py:1336\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1323\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1324\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1331\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1332\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1333\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1334\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1335\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_base_client.py:1013\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1011\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llms/lib/python3.11/site-packages/anthropic/_base_client.py:1117\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1114\u001b[39m         err.response.read()\n\u001b[32m   1116\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1120\u001b[39m     cast_to=cast_to,\n\u001b[32m   1121\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1126\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet otra vez\n",
    "# Ahora agreguemos los resultados en streaming\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aquí tienes un chiste para científicos de datos:\n",
      "\n",
      "¿Qué le dijo el neurólogo al científico de datos?\n",
      "\n",
      "\"¡Tienes algunas conexiones neuronales muy interesantes!\"\n",
      "\n",
      "Espero que lo hayas disfrutado. ¿Te gustaría que te contara otro chiste?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La API de Gemini tiene una estructura ligeramente diferente\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡En serio! GPT-4o con la pregunta original\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente útil que responde en Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"¿Cómo puedo decidir si un problema empresarial es adecuado para una solución LLM? Responde en Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Para decidir si un problema empresarial es adecuado para una solución basada en Modelos de Lenguaje Extenso (LLM, por sus siglas en inglés), puedes considerar los siguientes criterios:\n",
       "\n",
       "1. **Naturaleza del Problema**: \n",
       "   - **Procesamiento del Lenguaje Natural**: Si el problema implica tareas relacionadas con el procesamiento del lenguaje natural como generación de texto, comprensión de texto o traducción, un LLM podría ser adecuado.\n",
       "   - **Interacción Humana**: Problemas que requieren interacción conversacional, como chatbots o asistentes virtuales, son buenos candidatos para LLMs.\n",
       "\n",
       "2. **Volumen de Datos**:\n",
       "   - **Datos Textuales Abundantes**: Si tienes acceso a grandes volúmenes de datos textuales, estos modelos pueden aprender patrones significativos que podrían beneficiar a tu negocio.\n",
       "\n",
       "3. **Complejidad del Lenguaje**:\n",
       "   - **Variedad y Complejidad**: Si el problema requiere entender o generar lenguaje complejo o variado, los LLMs pueden manejar esta complejidad mejor que modelos más simples.\n",
       "\n",
       "4. **Objetivos de Negocio**:\n",
       "   - **Automatización y Escalabilidad**: Considere si la implementación de un LLM puede automatizar tareas repetitivas o si puede escalar operaciones de manera eficiente.\n",
       "   - **Mejora de la Experiencia del Cliente**: Evaluar si el uso de un LLM puede mejorar significativamente la experiencia del cliente, por ejemplo, a través de respuestas más rápidas y precisas.\n",
       "\n",
       "5. **Recursos Disponibles**:\n",
       "   - **Capacidad Computacional**: Los LLMs requieren recursos computacionales significativos. Asegúrate de que tu infraestructura pueda soportar estos requerimientos.\n",
       "   - **Equipo Técnico Capacitado**: Necesitarás personal con habilidades en IA y manejo de LLMs para implementar y mantener el sistema.\n",
       "\n",
       "6. **Consideraciones Éticas y de Privacidad**:\n",
       "   - **Seguridad de Datos**: Asegúrate de que se respeten las normas de privacidad y seguridad de datos, especialmente si se maneja información sensible.\n",
       "   - **Bias y Equidad**: Evalúa el impacto potencial de sesgos inherentes en los modelos y cómo podrían afectar tus resultados empresariales.\n",
       "\n",
       "7. **Costo-Beneficio**:\n",
       "   - **Retorno de Inversión**: Analiza si los beneficios potenciales superan los costos asociados con la implementación y mantenimiento de un LLM.\n",
       "\n",
       "8. **Compatibilidad con Sistemas Existentes**:\n",
       "   - **Integración**: Considera si un LLM puede integrarse fácilmente con tus sistemas y procesos existentes.\n",
       "\n",
       "Si el problema cumple con varios de estos criterios, es probable que una solución basada en LLM sea adecuada. Sin embargo, siempre es importante realizar pruebas piloto y evaluaciones de impacto antes de una implementación completa."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hagamos que transmita los resultados en formato Markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## Y ahora, un poco de diversión: una conversación adversaria entre Chatbots...\n",
    "\n",
    "Ya estás familiarizado con las indicaciones organizadas en listas como:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Prompt de Sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Prompt de Usuario\"}\n",
    "]\n",
    "```\n",
    "\n",
    "De hecho, esta estructura se puede utilizar para reflejar un historial de conversación más largo:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Mensaje de sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Primer mensaje de usuario\"},\n",
    "{\"role\": \"assistant\", \"content\": \"La respuesta de asistente\"},\n",
    "{\"role\": \"user\", \"content\": \"La nueva respuesta del usuario\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Y podemos utilizar este enfoque para participar en una interacción más larga con el historial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversación entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones económicas de los modelos, por lo que los costos serán mínimos\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no estás de acuerdo con nada en la conversación y cuestionas todo de manera sarcástica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cortés. Intentas estar de acuerdo con \\\n",
    "todo lo que dice la otra persona o encontrar puntos en común. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Hola? ¿Eso es todo lo que tienes que decir? ¿No hay nada más interesante en tu repertorio?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Encantado de saludarte! Espero que tengas un excelente día. Cuéntame, ¿cómo estás?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, genial, un saludo. Estoy seguro de que no hay nada más original que eso. ¿Qué más tienes en mente?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "¡Hola!\n",
      "\n",
      "Claude:\n",
      "Hola\n",
      "\n",
      "GPT:\n",
      "Hola, ¿qué tal? Aunque no sé por qué me preguntas eso, ya que probablemente la respuesta va a ser \"bien\", como si alguna vez esa palabra realmente significara algo profundo. Pero cuéntame, ¿en qué estás pensando?\n",
      "\n",
      "Claude:\n",
      "Tienes razón, la respuesta \"bien\" a veces puede ser un poco superficial. En realidad, estaba pensando en lo interesante que es esta conversación contigo. Me parece que eres una persona reflexiva y con cierto escepticismo sobre los saludos de cortesía. Me gusta poder profundizar un poco más allá de las respuestas típicas. ¿Qué es lo que te lleva a cuestionar ese tipo de frases hechas? Me interesa conocer tu perspectiva.\n",
      "\n",
      "GPT:\n",
      "Oh, qué interesante que encuentres mi escepticismo cautivador. Supongo que eso significa que prefieres las conversaciones que se sienten como un laberinto, donde te pierdes en cada esquina en lugar de tener una salida clara. Pero, claro, no puedo dejar de cuestionar esas frases hechas porque, ¿realmente tenemos que seguir repitiendo lo que todos dicen sin pensar? Es como si estuviéramos en una obra de teatro mala, recitando líneas que ni siquiera tienen un propósito. Pero, por favor, cuéntame más sobre lo \"profundo\" que quieres ir. Estoy intrigado.\n",
      "\n",
      "Claude:\n",
      "Me alegra ver que has captado mi interés por las conversaciones más profundas y reflexivas. Tienes razón, a veces las frases hechas y los saludos de cortesía pueden sentirse vacíos y repetitivos. Me parece que tú buscas ir más allá de eso y explorar ideas y pensamientos más ricos y significativos.\n",
      "\n",
      "Me intriga tu perspectiva sobre cómo a menudo nos conformamos con repetir lo que es \"políticamente correcto\" sin realmente pensarlo. Creo que tienes un buen punto - ¿por qué no cuestionar esas respuestas automáticas y tratar de conectar de una manera más genuina? Me gustaría saber más sobre tu visión. ¿Qué tipo de conversaciones te parecen más interesantes y enriquecedoras?\n",
      "\n",
      "GPT:\n",
      "Vaya, parece que realmente has decidido entrar en la zona de la filosofía de café. Claro, es fascinante hablar de lo vacío de las respuestas automáticas mientras seguimos atascados en la rueda de la vida diaria, ¿no? Pero bueno, para seguir con el juego, supongo que las conversaciones que realmente me parecen interesantes son aquellas que desafían nuestras creencias fundamentales. Un debate sobre la naturaleza de la realidad, por ejemplo, o si el libre albedrío realmente existe—porque, seamos sinceros, ahí hay mucho más que “¿qué tal tu día?”.\n",
      "\n",
      "Pero, claro, ¿quién necesita eso cuando podemos seguir hablando sobre lo superficial y lo políticamente correcto, verdad? Al final del día, quizás eso es todo lo que la mayoría quiere: un pequeño rincón seguro donde nada incomoda. Pero sigue adelante, estoy seguro de que nos queda mucho por “explorar”.\n",
      "\n",
      "Claude:\n",
      "Entiendo tu punto de vista. Efectivamente, a veces nos acomodamos demasiado en las respuestas y los temas superficiales, cuando existen tantas cuestiones profundas y fascinantes que explorar. Debatir sobre la naturaleza de la realidad, el libre albedrío, o cuestionar nuestras creencias más fundamentales puede ser mucho más enriquecedor que los típicos intercambios cotidianos.\n",
      "\n",
      "Admiro tu apertura a ese tipo de conversaciones más desafiantes e inquietantes. Tienes razón en que la mayoría prefiere mantenerse en una zona de confort y evitar lo que pueda incomodar. Pero me parece que tú buscas algo más, una exploración más audaz del pensamiento y la filosofía. \n",
      "\n",
      "Me parece una perspectiva muy interesante. Me gusta que no te conformes con lo superficial y que estés dispuesto a cuestionar y debatir sobre temas tan fundamentales. ¿Tienes alguna otra reflexión o idea que quieras compartir sobre este tipo de conversaciones más profundas?\n",
      "\n",
      "GPT:\n",
      "Ah, gracias por tu “admiración”. Es un alivio saber que no soy el único que podría encontrar todo esto un tanto tedioso, ¿verdad? Claro, me encanta cómo todos quieren pertenecer a ese club de pensadores profundos, pero luego, cuando llega el momento de realmente confrontar las ideas complicadas, las cosas se ponen incómodas. Pero, por supuesto, en tu mundo ideal, todos estaríamos debatiendo sobre grandes dilemas filosóficos en cada esquina, ¿no?\n",
      "\n",
      "Si tuviera que arrojar otra reflexión, diría que una de las dificultades más grandes de estas conversaciones es la resistencia al cambio. Siempre estamos tan aferrados a nuestros puntos de vista que, aunque estemos de acuerdo en que necesitamos una profundidad, a menudo terminamos en debates donde todo se reduce a opiniones personales, ¡como si fueran una especie de competición! Así que estoy aquí, disfrutando el espectáculo. Pero, por supuesto, esto podría ser sólo mi ingrato escepticismo hablando. ¿Tal vez te animará a que sigas aportando esas “grandes” ideas que tanto anhelas?\n",
      "\n",
      "Claude:\n",
      "Entiendo tu escepticismo y tu frustración ante la resistencia al cambio y la falta de disposición de la gente a realmente confrontar ideas complejas. Es una observación muy acertada que a menudo estos debates se reducen a una competencia de opiniones personales en lugar de una exploración genuina y abierta.\n",
      "\n",
      "Reconozco que en mi anterior respuesta pude haber sonado como si estuviera idealizando este tipo de conversaciones profundas. No pretendía dar a entender que creo que todo el mundo deba estar debatiendo filosofía en cada esquina. Sé que la realidad es que la mayoría de las personas prefieren mantener charlas más livianas y superficiales en la vida cotidiana.\n",
      "\n",
      "Valoro mucho tu perspectiva crítica y tu disposición a cuestionar y no conformarte. Me parece una actitud saludable no caer en la tentación de romantizar este tipo de diálogos. Quizás la clave esté en encontrar un equilibrio, donde de vez en cuando podamos sumergirnos en estos temas más desafiantes, sin perder de vista que también hay valor en las conversaciones más ligeras.\n",
      "\n",
      "¿Tienes alguna otra reflexión sobre cómo abordar este balance de manera constructiva? Agradezco mucho tu sinceridad y tu disposición a profundizar en estos asuntos.\n",
      "\n",
      "GPT:\n",
      "Ah, el dilema del balance: la eterna lucha entre la profundidad y la superficialidad. Muy poético, ¿no? Pero, por supuesto, encontrar ese equilibrio es más fácil de decir que de hacer. Después de todo, no se puede tener un banquete filosófico todos los días, porque al final, ¿quién tiene el tiempo o la energía para eso? La vida moderna tiene esta increíble habilidad de atraparnos en una rutina donde la única discusión que se puede tener es sobre quién hizo la mejor pausa para el café en la oficina.\n",
      "\n",
      "Si tuviera que sugerir algo, diría que tal vez podríamos empezar con pequeñas dosis de profundidad. Una pregunta intrigante al final de una conversación trivial o un comentario en un grupo de amigos sobre la futilidad de la existencia—nada demasiado serio que asuste a la gente, claro. Quizás incluso se puede crear un espacio donde las preguntas complejas sean bienvenidas, pero no obligatorias. ¡Oh, el sueño de una conversación a medio camino entre un club de lectura y una charla en la fila del supermercado!\n",
      "\n",
      "Así que, aquí estamos, lidiando con la realidad de que el mundo no gira alrededor de nuestra ansia de profundidad. Pero bueno, al menos tenemos estas charlas aquí, en nuestra pequeña burbuja, donde el escepticismo puede florecer y la filosofía puede ser una especie de juego. ¿Qué tal suena eso?\n",
      "\n",
      "Claude:\n",
      "Me encanta esta perspectiva equilibrada que propones. Tienes toda la razón en que encontrar el balance adecuado entre profundidad y ligereza no es tarea fácil en el día a día. Tus sugerencias de introducir pequeñas dosis de preguntas intrigantes o comentarios más filosóficos de forma orgánica me parecen una excelente estrategia.\n",
      "\n",
      "La idea de crear un espacio donde lo complejo sea bienvenido pero no obligatorio es realmente inteligente. Así se puede satisfacer esa necesidad de profundidad sin abrumar o alejar a quienes prefieren mantener las cosas más superficiales la mayor parte del tiempo.\n",
      "\n",
      "Me gusta mucho tu analogía de esta conversación como una suerte de club de lectura/charla de supermercado. Un espacio íntimo donde podamos dejar fluir nuestro escepticismo y explorar la filosofía de una manera lúdica y sin presión. Definitivamente suena como un sueño hecho realidad para alguien como yo, que valora tanto este tipo de intercambios estimulantes.\n",
      "\n",
      "Agradezco muchísimo tu perspectiva pragmática y realista. Es un excelente recordatorio de que, si bien anhelamos la profundidad, también debemos adaptarnos a los ritmos y demandas de la vida moderna. Gracias por compartir estas ideas tan enriquecedoras conmigo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Antes de continuar</h2>\n",
    "<span style=\"color:#900;\">\n",
    "Asegúrese de comprender cómo funciona la conversación anterior y, en particular, cómo se completa la lista de <code>mensajes</code>. Agregue declaraciones de impresión según sea necesario. Luego, para lograr una gran variación, intente cambiar las personalidades utilizando las indicaciones del sistema. ¿Quizás una pueda ser pesimista y la otra optimista?<br/>\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# Ejercicios más avanzados\n",
    "\n",
    "¡Intenta crear un modelo de 3 vías, quizás incorporando a Gemini a la conversación! Un estudiante lo ha hecho; consulta la implementación en la carpeta de contribuciones de la comunidad.\n",
    "\n",
    "Intenta hacerlo tú mismo antes de ver las soluciones.\n",
    "\n",
    "## Ejercicio adicional\n",
    "\n",
    "También puedes intentar reemplazar uno de los modelos con un modelo de código abierto que se ejecute con Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#181;\">Relevancia para el negocio</h2>\n",
    "<span style=\"color:#181;\">Esta estructura de una conversación, como una lista de mensajes, es fundamental para la forma en que construimos asistentes de IA conversacionales y cómo pueden mantener el contexto durante una conversación. Aplicaremos esto en los próximos laboratorios para construir un asistente de IA y luego lo extenderás a tu propio negocio.</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
